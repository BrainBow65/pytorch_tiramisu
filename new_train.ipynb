{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb93218",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc1a07a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from datasets import camvid\n",
    "from datasets import joint_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac27ad",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9215e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMVID_PATH = Path('SegNet-Tutorial/CamVid')\n",
    "RESULTS_PATH = Path('.results/')\n",
    "WEIGHTS_PATH = Path('.weights/')\n",
    "RESULTS_PATH.mkdir(exist_ok=True)\n",
    "WEIGHTS_PATH.mkdir(exist_ok=True)\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8557d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x76eaa91d2a80>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=camvid.mean, std=camvid.std)\n",
    "train_joint_transformer = transforms.Compose([\n",
    "    joint_transforms.JointRandomCrop(224), # crop 224-by-224 section\n",
    "    joint_transforms.JointRandomHorizontalFlip() # flip with 50% probability\n",
    "])\n",
    "\n",
    "train_dset = camvid.CamVid(CAMVID_PATH, 'train',\n",
    "      joint_transform=train_joint_transformer,\n",
    "      transform=transforms.Compose([\n",
    "          transforms.ToTensor(),\n",
    "          normalize,\n",
    "    ]))\n",
    "train_loader = torch.utils.data.DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dset = camvid.CamVid(\n",
    "    CAMVID_PATH, 'val', joint_transform=None,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]))\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dset = camvid.CamVid(\n",
    "    CAMVID_PATH, 'test', joint_transform=None,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa65777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
